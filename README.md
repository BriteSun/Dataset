# Dataset
I have found these dataset in research papers.



## Image

### Classification or Recognition or Generative

* MS COCO

	<http://mscoco.org/dataset/#overview>

* NVIDIA food Image classification

	<https://github.com/corona10/FoodDataSet>

* CIFAR-10, CIFAR-100

	<https://www.cs.toronto.edu/~kriz/cifar.html>

* Large-scale CelebFaces Attributes (CelebA) Dataset

	<http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html>

* Street View House Numbers (SVHN)

	<http://ufldl.stanford.edu/housenumbers/>

* MNIST

	<http://yann.lecun.com/exdb/mnist/>

* Facial Database

	<http://www.face-rec.org/databases/>

* Simple Vector Drawing Datasets

	<https://github.com/hardmaru/sketch-rnn-datasets>

* Places2 (공간 사진, 정보 데이터)

	<http://places2.csail.mit.edu/download.html>



### Medical

* Lung cancer dataset

	<https://luna.grand-challenge.org>

	<https://www.kaggle.com/c/data-science-bowl-2017>

* Brain tumor dataset

	<http://braintumorsegmentation.org>

* Breast cancer dataset (kaggle)

	<https://www.kaggle.com/uciml/breast-cancer-wisconsin-data>

* The cancer image archive

	<http://www.cancerimagingarchive.net>

* Mammograpy dataset

	<http://marathon.csee.usf.edu/Mammography/Database.html>

* Bio Image Dataset @ IIIT Delhi

	<http://www.iab-rubric.org/resources.html>



## Text

### Machine Translation

* StatMT(Machine Translation, summarization 등의 태스크를 위한 데이터셋으로 나라-나라 쌍의 데이터셋입니다.)

	<http://www.statmt.org/wmt14/translation-task.html>

	<http://www.statmt.org/wmt15/translation-task.html>
	
	<http://www.statmt.org/wmt16/translation-task.html>
	
	<http://www.statmt.org/wmt17/translation-task.html>

* UN parallel Corpus

	<https://conferences.unite.un.org/UNCorpus>


### Short text

* Tweet data, a subset of TREC 2011 microblog track

	<http://trec.nist.gov/data/tweets/>

* Title data, including news titles with class labels from some news websites

	<http://www.sogou.com/al>


### QA

* bAbI dataset (Facebook Question Answering)

	<https://research.facebook.com/research/babi/>

* Question/Answering(빈칸추론문제) pairs using CNN/Daily Mail articles

	<https://github.com/deepmind/rc-data>

* Stanford Question Answering Dataset

	<https://rajpurkar.github.io/SQuAD-explorer/>

* Visual Question Answering 

	* CLEVR: A Diagnostic Dataset for 
Compositional Language and Elementary Visual Reasoning
		
		<http://cs.stanford.edu/people/jcjohns/clevr/>


### Word Embedding

* Word2Vec에 쓰인 데이터셋(위키피디아, WMT11 등)
	<https://code.google.com/archive/p/word2vec/>

* Fast Text pre-trained vector set

	<https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md>


### Sentiment Analysis

* Stanford Sentiment Treebank(SST)

	<http://nlp.stanford.edu/sentiment/>



## Sound

* Nottingham music dataset

	<https://www-labs.iro.umontreal.ca/~lisa/deep/data/>



## Knowledge Base

* Freebase

	<https://datahub.io/ko_KR/dataset/freebase>

* Wordnet

	<https://wordnet.princeton.edu/>
	
* Microsoft Concept Graph

	<https://concept.msra.cn/Home/Download>

* DBPedia Dataset

	The DBpedia data set uses a large multi-domain ontology which has been derived from Wikipedia as well as  localized versions of DBpedia in more than 100 languages. 

	<http://wiki.dbpedia.org/services-resources/datasets/dbpedia-datasets>


* Yago

	YAGO3 is a huge semantic knowledge base, derived from Wikipedia WordNet and GeoNames.

	<https://datahub.io/ko_KR/dataset/yago>



## Pre-trained Model
* Word2Vect
	
	<https://code.google.com/archive/p/word2vec/>

* GloVe

	<https://nlp.stanford.edu/projects/glove/>

* FastText

	<https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md>



## ETC.

* Google sentence compression(Google에서 문장을 정형화 한 데이터입니다.)

	<http://storage.googleapis.com/sentencecomp/compression-data.json>

* 논문 bibliography 데이터셋

	<https://aminer.org/citation>

	<http://dblp.uni-trier.de/>

* Titanic survivors dataset

	<https://goo.gl/P9CMFY>

* Obama’s political speeches

	<https://github.com/samim23/obama-rnn>

* Coil-20

	<http://www.cs.columbia.edu/CAVE/software/softlib/coil-20.php>

